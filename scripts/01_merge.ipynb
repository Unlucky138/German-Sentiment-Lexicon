{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76aa745",
   "metadata": {},
   "source": [
    "# Script für die Zusammenführung der verschiedenen deutschen Sentiment Lexika zu einer CSV Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a55a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging SentiWS Positive and Negative txt files \n",
    "def combine_files(file1, file2, output_file):\n",
    "\n",
    "    with open(file1, 'r', encoding='utf-8') as f1:\n",
    "        lines1 = f1.readlines()\n",
    "\n",
    "    with open(file2, 'r', encoding='utf-8') as f2:\n",
    "        lines2 = f2.readlines()\n",
    "\n",
    "    combined_lines = lines1 + lines2\n",
    "\n",
    "    # sort and remove duplicates\n",
    "    combined_lines = list(set(combined_lines))\n",
    "    combined_lines.sort()  \n",
    "\n",
    "    # Writing the combined lines to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as out:\n",
    "        out.writelines(combined_lines)\n",
    "\n",
    "# paths to the input files and output file\n",
    "file_negative = 'SentiWS_v2.0_Negative.txt'\n",
    "file_positive = 'SentiWS_v2.0_Positive.txt'\n",
    "output_combined = 'SentiWS_v2.0_combined.txt'\n",
    "\n",
    "combine_files(file_negative, file_positive, output_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e08616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging GermanPolarityClues Positives and Negatives txt files into a single txt\n",
    "file_negative = 'GermanPolarityClues-Negative-21042012.tsv'\n",
    "file_positive = 'GermanPolarityClues-Positive-21042012.tsv'\n",
    "\n",
    "# Column names for the file\n",
    "column_names = [\"Wortform\", \"Lemma\", \"POS\", \"Polarität\", \"Sentimentwert\", \"Quelle\"]\n",
    "\n",
    "df_neg = pd.read_csv(file_negative, sep='\\t', header=None, names=column_names)\n",
    "df_pos = pd.read_csv(file_positive, sep='\\t', header=None, names=column_names)\n",
    "\n",
    "# Combine the DataFrames\n",
    "df_combined = pd.concat([df_neg, df_pos], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new TSV file\n",
    "output_file = 'GermanPolarityClues_combined.tsv'\n",
    "df_combined.to_csv(output_file, sep='\\t', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e3b66",
   "metadata": {},
   "source": [
    "Nachdem die Lexika, die in mehreren Dateien vorlagen, zusammengeführt wurden, geht es jetzt darum, alle bisherigen Lexika in eine große CSV-Datei zu überführen. Dabei soll am besten keine Information verloren gehen und das Wort als eine Art ID verwendet werden. Die CSV-Datei soll danach ungefährt so aussehen:\n",
    "| Wort | Worttyp | SentiWS | PolArt | GermanPolartyClues | Morph | MLSA | UniSent | AffNorms | Etc. (andere Lexika, evtl. Flexionsformen)\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Beispiel | NN | 0.355 | NEG=0.7 | negative-/-0.058 | NEG | -0.1025 | -1 | 7.786 | |\n",
    "\n",
    "Weitere interessante Lexika umfassen: \n",
    "- **AffDict** (Schröder, 2011)  \n",
    "- **Aff-Meaning** (Ambrasat et al., 2014)  \n",
    "- **ALPIN** (Kolb et al., 2021)  \n",
    "- **ANGST** (Schmidtke et al., 2014)  \n",
    "- **BAWL-R** (Võ et al., 2009)  \n",
    "- **SePL** (Rill et al., 2012) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# 1. SentiWS\n",
    "sentiws = []\n",
    "with open(\"SentiWS_v2.0_combined.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) < 3: continue\n",
    "        word_tag, polarity, _ = parts\n",
    "        word, worttyp = word_tag.split(\"|\")\n",
    "        sentiws.append({\n",
    "            \"Wort\": word.lower(),\n",
    "            \"Worttyp\": worttyp,\n",
    "            \"SentiWS\": float(polarity)\n",
    "        })\n",
    "sentiws_df = pd.DataFrame(sentiws)\n",
    "\n",
    "# 2. PolArt\n",
    "polart = []\n",
    "with open(\"PolArt_lexicon.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 3:\n",
    "            word, polarity_full, _ = parts\n",
    "            polart.append({\n",
    "                \"Wort\": word.lower(),\n",
    "                \"PolArt\": polarity_full\n",
    "            })\n",
    "polart_df = pd.DataFrame(polart).drop_duplicates(subset=\"Wort\")\n",
    "\n",
    "# 3. GermanPolarityClues\n",
    "gpc_df = pd.read_csv(\"GermanPolarityClues_combined.tsv\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "pos_map = {\n",
    "    \"NN\": \"NN\",\n",
    "    \"AD\": \"ADJX\",\n",
    "    \"VV\": \"VVINF\"\n",
    "}\n",
    "gpc_df[\"Worttyp\"] = gpc_df[\"POS\"].map(pos_map)\n",
    "gpc_df = gpc_df.drop_duplicates(subset=[\"Lemma\"])\n",
    "gpc_df = gpc_df.rename(columns={\n",
    "    \"Lemma\": \"Wort\",\n",
    "    \"Polarität\": \"GermanPolarityClues\"\n",
    "})\n",
    "gpc_df[\"Wort\"] = gpc_df[\"Wort\"].str.lower()\n",
    "gpc_df = gpc_df[[\"Wort\", \"Worttyp\", \"GermanPolarityClues\"]]\n",
    "\n",
    "# 4. Morph\n",
    "morph = []\n",
    "with open(\"Morph_combined.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) == 2:\n",
    "            word_with_type, polarity = parts\n",
    "            word = word_with_type.split(\"_\")[0]\n",
    "            morph.append({\n",
    "                \"Wort\": word.lower(),\n",
    "                \"Morph\": polarity\n",
    "            })\n",
    "morph_df = pd.DataFrame(morph).drop_duplicates(subset=\"Wort\")\n",
    "\n",
    "# 5. MLSA\n",
    "mlsa_entries = []\n",
    "tree = ET.parse(\"MLSA-presseRel.xml\")\n",
    "root = tree.getroot()\n",
    "for entry in root.findall(\"entry\"):\n",
    "    word = entry.findtext(\"term\")\n",
    "    mlsa = None\n",
    "    for opinion in entry.findall(\"opinion\"):\n",
    "        if \"MLSA\" in opinion.get(\"source\"):\n",
    "            polarity = float(opinion.get(\"polarity\"))\n",
    "            mlsa = polarity\n",
    "    mlsa_entries.append({\n",
    "        \"Wort\": word.lower(),\n",
    "        \"MLSA\": mlsa\n",
    "    })\n",
    "mlsa_df = pd.DataFrame(mlsa_entries).drop_duplicates(subset=\"Wort\")\n",
    "\n",
    "# 6. UniSent\n",
    "unisent = []\n",
    "with open(\"deu_unisent_lexicon.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) == 2:\n",
    "            word, polarity = parts\n",
    "            unisent.append({\n",
    "                \"Wort\": word.lower(),\n",
    "                \"UniSent\": int(polarity)\n",
    "            })\n",
    "unisent_df = pd.DataFrame(unisent).drop_duplicates(subset=\"Wort\")\n",
    "\n",
    "# 7. AffNorms\n",
    "affnorms_df = pd.read_csv(\"AffNorms_ratings.txt\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "affnorms_df[\"Wort\"] = affnorms_df[\"Word\"].str.lower()\n",
    "affnorms_df = affnorms_df.rename(columns={\"Val\": \"AffNorms_Val\"})\n",
    "affnorms_df = affnorms_df[[\"Wort\", \"AffNorms_Val\"]].drop_duplicates(subset=\"Wort\")\n",
    "\n",
    "# 8. ALPIN\n",
    "alpin = []\n",
    "with open(\"ALPIN_v1.0.csv\", encoding=\"utf-8\") as f:\n",
    "    next(f)  # Skip Header\n",
    "    for line in f:\n",
    "        word, tag, sentiment = line.strip().split(\",\")\n",
    "        # Exclude lines in which the word is a hyperlink\n",
    "        if word.startswith(\"http://\") or word.startswith(\"https://\"):\n",
    "            continue\n",
    "        alpin.append({\n",
    "            \"Wort\": word.lower(),\n",
    "            \"ALPIN_sentiment_scaled\": float(sentiment)\n",
    "        })\n",
    "alpin_df = pd.DataFrame(alpin)\n",
    "alpin_df = alpin_df.drop_duplicates(subset=[\"Wort\"])\n",
    "\n",
    "# 9. ANGST\n",
    "import openpyxl\n",
    "angst_df = pd.read_excel(\"ANGST.xlsx\", engine=\"openpyxl\")\n",
    "angst_df = angst_df.rename(columns={\"G-word\": \"Wort\", \"VAL_Mean\": \"ANGST_Valence\"})\n",
    "angst_df[\"Wort\"] = angst_df[\"Wort\"].str.lower()\n",
    "angst_df = angst_df[[\"Wort\", \"ANGST_Valence\"]].drop_duplicates(subset=\"Wort\")\n",
    "\n",
    "\n",
    "# 10. AffDict\n",
    "affdict_df = pd.read_excel(\"AffDict.xls\", engine=\"xlrd\")\n",
    "\n",
    "# only relevant columns\n",
    "affdict_df = affdict_df.rename(columns={\"German Word\": \"Wort\", \"E unisex\": \"AffDict_Eval\"})\n",
    "\n",
    "# remove rows with NaN in \"Wort\"\n",
    "affdict_df = affdict_df.dropna(subset=[\"Wort\"])\n",
    "\n",
    "# clean \"Wort\" column: strip whitespace, convert to lowercase, keep only alphabetic words\n",
    "affdict_df[\"Wort\"] = affdict_df[\"Wort\"].astype(str).str.strip().str.lower()\n",
    "affdict_df = affdict_df[affdict_df[\"Wort\"].str.fullmatch(r\"[a-zäöüß]+\")]\n",
    "\n",
    "affdict_df = affdict_df[[\"Wort\", \"AffDict_Eval\"]]\n",
    "\n",
    "\n",
    "# Merging all\n",
    "master_df = sentiws_df.copy()\n",
    "master_df[\"Wort\"] = master_df[\"Wort\"].str.lower()\n",
    "\n",
    "dfs_to_merge = [\n",
    "    polart_df, gpc_df, morph_df, mlsa_df,\n",
    "    unisent_df, affnorms_df, alpin_df, nrc_df,\n",
    "    angst_df, affdict_df\n",
    "]\n",
    "\n",
    "for df in dfs_to_merge:\n",
    "    master_df = master_df.merge(df, on=\"Wort\", how=\"outer\")\n",
    "\n",
    "# Cleaning\n",
    "master_df = master_df[master_df[\"Wort\"].notna()]  \n",
    "master_df[\"Wort\"] = master_df[\"Wort\"].astype(str) \n",
    "# remove rows where \"Wort\" does not start with a letter or number\n",
    "master_df = master_df[master_df[\"Wort\"].str.match(r\"^[0-9a-zäöüß]\")]  \n",
    "\n",
    "master_df = master_df.drop_duplicates(subset=\"Wort\")\n",
    "\n",
    "# sort and setting index\n",
    "master_df = master_df.sort_values(\"Wort\").reset_index(drop=True)\n",
    "master_df.insert(0, \"ID\", master_df.index)\n",
    "\n",
    "# Export\n",
    "master_df.to_csv(\"sentiment_lexika_merged_clean.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d56739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv(\"sentiment_lexika_merged_clean.csv\")\n",
    "\n",
    "# PolArt parsen\n",
    "def parse_polart(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    try:\n",
    "        typ, score = val.split(\"=\")\n",
    "        score = float(score)\n",
    "        if typ == \"NEG\":\n",
    "            return -score\n",
    "        elif typ == \"POS\":\n",
    "            return score\n",
    "        elif typ == \"NEU\":\n",
    "            return 0.0\n",
    "        else:\n",
    "            return np.nan  # INT, SHI... ignore\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['PolArt_num'] = df['PolArt'].apply(parse_polart)\n",
    "\n",
    "# GermanPolarityClues mapping\n",
    "gpc_map = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "df['GPC_num'] = df['GermanPolarityClues'].map(gpc_map)\n",
    "\n",
    "# Morph mapping\n",
    "morph_map = {'NEG': -1, 'NEU': 0, 'POS': 1}\n",
    "df['Morph_num'] = df['Morph'].map(morph_map)\n",
    "\n",
    "# scaler \n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# relevant columns to scale\n",
    "columns_to_scale = ['AffNorms_Val', 'ANGST_Valence', 'AffDict_Eval']\n",
    "existing_cols = [col for col in columns_to_scale if col in df.columns]\n",
    "\n",
    "scaled_data = scaler.fit_transform(df[existing_cols])\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=[col + \"_scaled\" for col in existing_cols])\n",
    "\n",
    "# combine all relevant columns\n",
    "final_df = pd.concat([\n",
    "    df[['ID', 'Wort']],\n",
    "    df[['SentiWS', 'PolArt_num', 'GPC_num', 'Morph_num', 'MLSA', 'UniSent', 'ALPIN_sentiment_scaled']],\n",
    "    scaled_df\n",
    "], axis=1)\n",
    "\n",
    "# export\n",
    "final_df.to_csv(\"sentiment_lexika_scaled_final.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ef293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Versuch der Anreicherung des Datensatzes (Worttyp für alle Wörter) mit spacy\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "# df = pd.read_csv(\"sentiment_lexika_merged.csv\")\n",
    "\n",
    "# # Funktion zur POS-Bestimmung mit spaCy\n",
    "# def detect_pos_spacy(word):\n",
    "#     doc = nlp(word)\n",
    "#     if doc and doc[0].pos_:\n",
    "#         return doc[0].pos_  # z. B. 'NOUN', 'ADJ', 'VERB', ...\n",
    "#     return None\n",
    "\n",
    "# # Neue Spalte immer befüllen \n",
    "# df[\"Worttyp_spacy\"] = df[\"Wort\"].apply(detect_pos_spacy)\n",
    "\n",
    "# # Mapping auf SentiWS-kompatible Kürzel\n",
    "# pos_map = {\n",
    "#     \"NOUN\": \"NN\",\n",
    "#     \"ADJ\": \"ADJX\",\n",
    "#     \"ADV\": \"ADJX\",\n",
    "#     \"VERB\": \"VVINF\",\n",
    "#     \"AUX\": \"VVINF\",\n",
    "# }\n",
    "# df[\"Worttyp_spacy_mapped\"] = df[\"Worttyp_spacy\"].map(pos_map)\n",
    "\n",
    "# # CSV speichern\n",
    "# df.to_csv(\"sentiment_lexika_with_spacy.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
